Quando in informatica si parla di data e ora, i valori del calendario e dell'orologio che
solitamente sono mostrati in forma di giorno, mese, anno, ore, secondi, in realtà vengono
rappresentati in modo completamente differente.
In particolare, nella maggior parte dei casi, quando si vuole rappresentare una certa data
e un certo orario, lo si fa con un numero intero, che rappresenta il numero di secondi che separano
quella particolare data e ora da un'altra che è conosciuta come epoca e che corrisponde
al primo gennaio 1970.
Una rappresentazione del genere semplifica tutte le operazioni di manipolazione e confronto
fra date, perché invece di operare su tanti numeri separati, permette di farlo su un unico
numero intero.
E quindi, ad esempio, la mezzanotte del giorno di Natale dell'anno 2021 sarà rappresentata
da gran parte dei programmi e dei sistemi operativi come 1.640.390.400 secondi dall'epoca.
Ora però, se si ragiona ad un livello un po' più basso, cioè più vicino al modo
in cui i computer trattano effettivamente le informazioni, più o meno qualsiasi dato
viene rappresentato in forma di codice binario, quindi per mezzo di una sequenza più o meno
lunga di cifre di 0 e 1.
E dunque, in informatica, date e ore vengono prima rappresentate in numero di secondi dall'epoca
e poi a sua volta questo numero di secondi viene invece rappresentato come una sequenza
di bit.
Tutto questo ragionamento per dire che esiste un limite alle date che possono essere rappresentate
in informatica e questo limite dipende dal numero di bit che si ha a disposizione.
Ad esempio, con un sistema 32 bit, architettura che ormai sta sparendo dai device più moderni
ma che è ancora largamente utilizzata in molte applicazioni sia professionali che
industriali, è possibile rappresentare al massimo i numeri fino a 2.147.483.647, riservando
un bit per indicare se il numero è un positivo o un negativo.
E se tale valore massimo viene calcolato come secondi dall'epoca, esso corrisponde alle
3.1407 del 19 gennaio 2038.
Arrivati a tale data e ora, quindi, in un sistema a 32 bit, l'aggiunta di un altro secondo
causerà un cosiddetto errore di rappresentazione e invece di portare ad un intero con un'unità
in più, porterà ad un numero negativo.
In pratica, un eventuale orologio a 32 bit, nell'arco di quel secondo, passerà dalle
3.1407 del 19 gennaio 2038 direttamente alle 20.45.52 del 13 dicembre 1901.
A questo particolare bug è stato dato il nome di Y2038, proprio perché se dovesse
verificarsi nella realtà, lo farebbe all'inizio dell'anno 2038.
E ovviamente, un qualsiasi sistema o programma che si trovasse coinvolto in un errore del
genere difficilmente riuscirebbe ad espletare correttamente le proprie funzioni, e un evento
del genere potrebbe portare a risultati davvero inaspettati.
Detta così, potrebbe sembrare una catastrofe, ma in realtà, come per il simile ma ben più
noto predecessore Millennium Bug, non è certo il caso di disperare per Y2038, poiché
essendo a conoscenza del problema, sono già state elaborate varie soluzioni, e si tratta
solo di implementarle prima dell'arrivo della fatidica data.
Già conosciuto fra ghiandetti ai lavori fin dal 2006, infatti, il problema della rappresentazione
massima di un intero in 32bit è salito agli onori della cronaca nel 2014, quando la pagina
del video Gangam Style, disponibile su YouTube, è andata in crash perché il contatore delle
visualizzazioni ha superato i 2 miliardi e 147 milioni, mandando quindi in tilt la variabile
a 32bit che non riusciva più a contenerne il valore.
Dal momento quindi che sia Google sia YouTube sono ancora in piedi e perfettamente funzionanti,
e Gangam Style ha ormai superato abbondantemente i 4 miliardi di visualizzazioni, appare chiaro
che la soluzione esiste e può essere applicata, e se tutti faranno la propria parte, come
è accaduto per il Millennium Bug, è molto probabile che nessuno di noi debba preoccuparsi
di cosa accadrà alle nostre variabili nel 2038.
Grazie per aver ascoltato Snippet, una rubrica di Pensieri in Codice. Scopri tanti altri
contenuti sul sito pensieriincodice.it
